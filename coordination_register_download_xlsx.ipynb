{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0ecd29ab2faa24e79e93bd60ef0ffd6095313ed658af1afd1fe7b016bef9443a6",
   "display_name": "Python 3.9.1 64-bit ('.venv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Downloading the full dataset as a spreadsheet (XLSX) and analysing with Pandas\n",
    "\n",
    "As we saw, when your search results in more than 10 000 organisations, it is not possible to get access to all of them through the search-API. Instead it is recommended to download the full dataset. This could be done in many ways, for instance through manually download on the [home page of the open data from the registry](https://data.brreg.no/enhetsregisteret/oppslag/enheter), or using curl as shown in the [API-documentation](https://data.brreg.no/enhetsregisteret/api/docs/index.html#enheter-lastned). Below, it is shown how to use Python to download and save the file, as well as how to read and analyse the file with Pandas.\n",
    "\n",
    "Pros:\n",
    "- already \"flattened\" and easy to work with for example with Python Pandas\n",
    "- the result-file is smaller than the JSON-file\n",
    "\n",
    "Cons:\n",
    "- normally requires to read the full file in memory before manipulating it/making a new file with a subset of the data\n",
    "- harder to combine with data from the API (different names for columns/elements)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As with the search-API, We use requests for getting data from the server"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "source": [
    "Since the size of the file is quite large, we will use a method where the file is downloaded by Python and written to a local file in chunks, instead of Python having to download and open the full file, before it writes it to a local file. Be aware that the files are delivered as compressed files, that can not be opened directly in an editor. One solution is to uncompress the file after it is downloaded, but as we will see, it is also possible for Python to use the compressed file directly when we use the Pandas-library."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laster ned XLSX:\n",
    "url = 'https://data.brreg.no/enhetsregisteret/api/enheter/lastned/regneark'\n",
    "headers = {'Accept': 'application/vnd.brreg.enhetsregisteret.enhet+vnd.openxmlformats-officedocument.spreadsheetml.sheet;charset=UTF-8'}\n",
    "session = requests.Session() # establish a session that is kept open during the transfer, instead of performing separate requests\n",
    "r = session.get(url, headers=headers, stream = True)\n",
    "r.raise_for_status()\n",
    "with open('er.xlsx','wb') as f:\n",
    "    for chunk in r.iter_content(1024*1024*10): # approx 10 MB in each chunck\n",
    "        f.write(chunk)"
   ]
  },
  {
   "source": [
    "## Part 3: Opening and analysing the file with Pandas\n",
    "Having downloaded the file, as we saw in the above section, is of little use if we don't get access to it. A good tool for opening large data files and analysing them, is Pandas. Pandas has method for many file types, and also handles the fact that they are compressed (gzip)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "source": [
    "The most common way to import Pandas, and numpy which is often necessary as it contains relevant data-types:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "In some cases, you also need to explicitly install openpyxl for pandas to be able to read files of type xlsx"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "source": [
    "We could just let pandas read and guess the datatype (dtype) of each column, but it saves us some time to have it specified already at the import. And now that the code is here, you don't have to type it manually ... :-)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_excel('er.xlsx', dtype={\n",
    "        'Organisasjonsnummer': str,\n",
    "        'Navn': str,\n",
    "        'Organisasjonsform.kode': 'category',\n",
    "        'Organisasjonsform.beskrivelse': 'category',\n",
    "        'Næringskode 1': str,\n",
    "        'Næringskode 1.beskrivelse': str,\n",
    "        'Næringskode 2': str,\n",
    "        'Næringskode 2.beskrivelse': str,\n",
    "        'Næringskode 3': str,\n",
    "        'Næringskode 3.beskrivelse': str,\n",
    "        'Hjelpeenhetskode': 'category',\n",
    "        'Hjelpeenhetskode.beskrivelse': 'category',\n",
    "        'Antall ansatte': np.int16,\n",
    "        'Hjemmeside': str,\n",
    "        'Postadresse.adresse': str,\n",
    "        'Postadresse.poststed': str,\n",
    "        'Postadresse.postnummer': str,\n",
    "        'Postadresse.kommune': str,\n",
    "        'Postadresse.kommunenummer': str,\n",
    "        'Postadresse.land': 'category',\n",
    "        'Postadresse.landkode': 'category',\n",
    "        'Forretningsadresse.adresse': str,\n",
    "        'Forretningsadresse.poststed': str,\n",
    "        'Forretningsadresse.postnummer': str,\n",
    "        'Forretningsadresse.kommune': str,\n",
    "        'Forretningsadresse.kommunenummer': str,\n",
    "        'Forretningsadresse.land': 'category',\n",
    "        'Forretningsadresse.landkode': 'category',\n",
    "        'Institusjonell sektorkode': 'category',\n",
    "        'Institusjonell sektorkode.beskrivelse': 'category',\n",
    "        'Siste innsendte årsregnskap': str, # klarte ikke konvertere til np.int16\n",
    "        'Registreringsdato i Enhetsregisteret': 'datetime64',\n",
    "        'Stiftelsesdato': str, # klarte ikke å konvertere til datetime64 - 1550-12-31 00:00:00\n",
    "        'FrivilligRegistrertIMvaregisteret': 'category',\n",
    "        'Registrert i MVA-registeret': 'category',\n",
    "        'Registrert i Frivillighetsregisteret': 'category',\n",
    "        'Registrert i Foretaksregisteret': 'category',\n",
    "        'Registrert i Stiftelsesregisteret': 'category',\n",
    "        'Konkurs': 'category',\n",
    "        'Under avvikling': 'category',\n",
    "        'Under tvangsavvikling eller tvangsoppløsning': 'category',\n",
    "        'Overordnet enhet i offentlig sektor': str,\n",
    "        'Målform': 'category' })"
   ]
  },
  {
   "source": [
    "Now we can get som info about the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "source": [
    "When exploring and setting up the relevant analysis, you could consider making a sample of the dataset, if it is slow to work with the full registry. Using frac=0.2 gives you a random sample of 20 % of the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sample)"
   ]
  },
  {
   "source": [
    "It might also be nice to save the sample as a file so that you don't need to wait for Python to read the full dataset the next time you work on your project. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.to"
   ]
  }
 ]
}